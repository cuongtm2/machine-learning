{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cuongtm2/machine-learning/blob/master/TF_training_Day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.7.0\n",
        "!pip install tensorflow-datasets==4.4.0 \n",
        "!pip install Pillow==8.4.0 \n",
        "!pip install pandas==1.3.4 \n",
        "!pip install numpy==1.21.4 \n",
        "!pip install scipy==1.7.3"
      ],
      "metadata": {
        "id": "bTIRwLc_z9FN",
        "outputId": "31863796-2697-4e07-864d-e0a7bbedd51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7.0\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl\n",
            "\u001b[K     \\ 665.5 MB 235 kB/s\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.46.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.26.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.37.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (14.0.1)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.21.6)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (0.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (2.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (4.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7.0) (1.0.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7.0) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7.0) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (4.11.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7.0) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7.0) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (2022.5.18.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7.0) (3.2.0)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220527125636\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220527125636:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220527125636\n",
            "Successfully installed gast-0.4.0 keras-2.7.0 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-datasets==4.4.0\n",
            "  Downloading tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (21.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.8.0)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (3.17.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (2.23.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (0.16.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (0.3.5.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (2.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (4.64.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets==4.4.0) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->tensorflow-datasets==4.4.0) (2.10)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets==4.4.0) (3.8.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets==4.4.0) (1.56.2)\n",
            "Installing collected packages: tensorflow-datasets\n",
            "  Attempting uninstall: tensorflow-datasets\n",
            "    Found existing installation: tensorflow-datasets 4.0.1\n",
            "    Uninstalling tensorflow-datasets-4.0.1:\n",
            "      Successfully uninstalled tensorflow-datasets-4.0.1\n",
            "Successfully installed tensorflow-datasets-4.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow==8.4.0\n",
            "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 8.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Pillow-8.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.3.4\n",
            "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.4) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.3.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.21.4\n",
            "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed numpy-1.21.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scipy==1.7.3\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7.3) (1.21.4)\n",
            "Installing collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rIapdYS3btiL"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTYDkvowFbCW",
        "outputId": "cee8c673-0588-451c-fa24-78e9531d05da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwOMTvwib9Th"
      },
      "outputs": [],
      "source": [
        "# Build and train a neural network to predict sunspot activity using\n",
        "# the Sunspots.csv dataset.\n",
        "#\n",
        "# Your neural network must have an MAE of 0.12 or less on the normalized dataset\n",
        "# for top marks.\n",
        "#\n",
        "# Code for normalizing the data is provided and should not be changed.\n",
        "#\n",
        "# At the bottom of this file, we provide  some testing\n",
        "# code in case you want to check your model.\n",
        "\n",
        "# Note: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jggQnB-YcB54"
      },
      "outputs": [],
      "source": [
        "url = 'https://storage.googleapis.com/download.tensorflow.org/data/Sunspots.csv'\n",
        "urllib.request.urlretrieve(url, 'sunspots.csv')\n",
        "\n",
        "time_step = []\n",
        "sunspots = []\n",
        "\n",
        "with open('sunspots.csv') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        sunspots.append(float(row[2]))\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "series = np.array(sunspots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf1Xk4omd-fq"
      },
      "outputs": [],
      "source": [
        "# This is the normalization function\n",
        "min = np.min(series)\n",
        "max = np.max(series)\n",
        "series -= min\n",
        "series /= max\n",
        "time = np.array(time_step)\n",
        "\n",
        "# The data should be split into training and validation sets at time step 3000\n",
        "# DO NOT CHANGE THIS CODE\n",
        "split_time = 3000\n",
        "time_train = time[:split_time]\n",
        "x_train = series[:split_time]\n",
        "time_valid = time[split_time:]\n",
        "x_valid = series[split_time:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mean = np.mean(series)\n",
        "# std = np.std(series)\n",
        "# series -= mean\n",
        "# series /= std\n",
        "# time = np.array(time_step)\n",
        "\n",
        "# # The data should be split into training and validation sets at time step 3000\n",
        "# # DO NOT CHANGE THIS CODE\n",
        "# split_time = 3000\n",
        "# time_train = time[:split_time]\n",
        "# x_train = series[:split_time]\n",
        "# time_valid = time[split_time:]\n",
        "# x_valid = series[split_time:]"
      ],
      "metadata": {
        "id": "PXp74Z_VKWmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-dCjDxDcF_Q"
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObfPlru6cgCS"
      },
      "outputs": [],
      "source": [
        "# Parameter 1\n",
        "window_size = 30\n",
        "batch_size = 32\n",
        "shuffle_buffer_size = 1000\n",
        "train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size,\n",
        "                              shuffle_buffer=shuffle_buffer_size)\n",
        "val_set = windowed_dataset(x_valid, window_size=window_size, batch_size=batch_size,\n",
        "                              shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "\n",
        "# Parameter 2\n",
        "# window_size = 100\n",
        "# batch_size = 512\n",
        "# shuffle_buffer_size = 1000\n",
        "# train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Model 2\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n",
        "  tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "  tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "  tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "  tf.keras.layers.Dense(1)\n",
        "  # tf.keras.layers.Lambda(lambda x: x * 400)\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPn5z8T0uHdP",
        "outputId": "e6da0713-aa72-4834-cce8-54d562e0dad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, None, 64)          384       \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, None, 128)        66048     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, None, 128)        98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 64)          8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 64)          0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 32)          2080      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 32)          0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, None, 16)          528       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, None, 1)           17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 176,129\n",
            "Trainable params: 176,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient = 10\n",
        "save_path = '/content/drive/MyDrive/tf_certification/model5.h5'\n",
        "callbacks_list = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=patient, \n",
        "        mode='min', \n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor = 'val_loss', \n",
        "        factor = 0.5, \n",
        "        patience = patient / 2, \n",
        "        min_lr=1e-6,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    ),\n",
        "    ModelCheckpoint(save_path, \n",
        "                    monitor=\"val_mae\",\n",
        "                    mode=\"min\",\n",
        "                    verbose=1,\n",
        "                    save_best_only=True)\n",
        "    ]"
      ],
      "metadata": {
        "id": "nfLMoUDGEyTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model 2 - parameter 1\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer='adam', metrics=['mae'])\n",
        "history = model.fit(train_set, epochs=200, verbose=1,validation_data=val_set,\n",
        "                    callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRF6t-zErSjB",
        "outputId": "9926f992-69df-44dc-f9aa-d1d4b79642bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "     93/Unknown - 16s 81ms/step - loss: 0.0058 - mae: 0.0752\n",
            "Epoch 1: val_mae improved from inf to 0.05803, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 18s 108ms/step - loss: 0.0058 - mae: 0.0752 - val_loss: 0.0032 - val_mae: 0.0580 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0028 - mae: 0.0536\n",
            "Epoch 2: val_mae improved from 0.05803 to 0.05701, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 90ms/step - loss: 0.0028 - mae: 0.0536 - val_loss: 0.0033 - val_mae: 0.0570 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0022 - mae: 0.0474\n",
            "Epoch 3: val_mae improved from 0.05701 to 0.05276, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 0.0022 - mae: 0.0474 - val_loss: 0.0031 - val_mae: 0.0528 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0018 - mae: 0.0429\n",
            "Epoch 4: val_mae did not improve from 0.05276\n",
            "93/93 [==============================] - 8s 82ms/step - loss: 0.0018 - mae: 0.0429 - val_loss: 0.0031 - val_mae: 0.0534 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0015 - mae: 0.0389\n",
            "Epoch 5: val_mae improved from 0.05276 to 0.04702, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 0.0015 - mae: 0.0389 - val_loss: 0.0024 - val_mae: 0.0470 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0013 - mae: 0.0361\n",
            "Epoch 6: val_mae did not improve from 0.04702\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0013 - mae: 0.0361 - val_loss: 0.0023 - val_mae: 0.0481 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0010 - mae: 0.0325\n",
            "Epoch 7: val_mae did not improve from 0.04702\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0010 - mae: 0.0325 - val_loss: 0.0025 - val_mae: 0.0484 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 8.5334e-04 - mae: 0.0289\n",
            "Epoch 8: val_mae improved from 0.04702 to 0.03954, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 8.5334e-04 - mae: 0.0289 - val_loss: 0.0017 - val_mae: 0.0395 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 6.9396e-04 - mae: 0.0255\n",
            "Epoch 9: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 6.9396e-04 - mae: 0.0255 - val_loss: 0.0020 - val_mae: 0.0425 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 6.3198e-04 - mae: 0.0239\n",
            "Epoch 10: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 6.3198e-04 - mae: 0.0239 - val_loss: 0.0021 - val_mae: 0.0463 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.8463e-04 - mae: 0.0227\n",
            "Epoch 11: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 5.8463e-04 - mae: 0.0227 - val_loss: 0.0018 - val_mae: 0.0405 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.6554e-04 - mae: 0.0220\n",
            "Epoch 12: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 5.6554e-04 - mae: 0.0220 - val_loss: 0.0018 - val_mae: 0.0398 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.3387e-04 - mae: 0.0213\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 13: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 5.3387e-04 - mae: 0.0213 - val_loss: 0.0017 - val_mae: 0.0412 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 5.0200e-04 - mae: 0.0206\n",
            "Epoch 14: val_mae did not improve from 0.03954\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 5.0200e-04 - mae: 0.0206 - val_loss: 0.0017 - val_mae: 0.0401 - lr: 5.0000e-04\n",
            "Epoch 15/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.8564e-04 - mae: 0.0202\n",
            "Epoch 15: val_mae improved from 0.03954 to 0.03853, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 4.8564e-04 - mae: 0.0202 - val_loss: 0.0016 - val_mae: 0.0385 - lr: 5.0000e-04\n",
            "Epoch 16/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.7768e-04 - mae: 0.0200\n",
            "Epoch 16: val_mae did not improve from 0.03853\n",
            "93/93 [==============================] - 8s 82ms/step - loss: 4.7768e-04 - mae: 0.0200 - val_loss: 0.0016 - val_mae: 0.0396 - lr: 5.0000e-04\n",
            "Epoch 17/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.6811e-04 - mae: 0.0197\n",
            "Epoch 17: val_mae improved from 0.03853 to 0.03648, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 4.6811e-04 - mae: 0.0197 - val_loss: 0.0014 - val_mae: 0.0365 - lr: 5.0000e-04\n",
            "Epoch 18/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.6878e-04 - mae: 0.0196\n",
            "Epoch 18: val_mae improved from 0.03648 to 0.03583, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 4.6878e-04 - mae: 0.0196 - val_loss: 0.0014 - val_mae: 0.0358 - lr: 5.0000e-04\n",
            "Epoch 19/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.5600e-04 - mae: 0.0194\n",
            "Epoch 19: val_mae improved from 0.03583 to 0.03311, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 4.5600e-04 - mae: 0.0194 - val_loss: 0.0012 - val_mae: 0.0331 - lr: 5.0000e-04\n",
            "Epoch 20/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.4691e-04 - mae: 0.0193\n",
            "Epoch 20: val_mae did not improve from 0.03311\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 4.4691e-04 - mae: 0.0193 - val_loss: 0.0012 - val_mae: 0.0338 - lr: 5.0000e-04\n",
            "Epoch 21/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.2784e-04 - mae: 0.0188\n",
            "Epoch 21: val_mae improved from 0.03311 to 0.03260, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 4.2784e-04 - mae: 0.0188 - val_loss: 0.0011 - val_mae: 0.0326 - lr: 5.0000e-04\n",
            "Epoch 22/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.1670e-04 - mae: 0.0186\n",
            "Epoch 22: val_mae did not improve from 0.03260\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 4.1670e-04 - mae: 0.0186 - val_loss: 0.0011 - val_mae: 0.0330 - lr: 5.0000e-04\n",
            "Epoch 23/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.1343e-04 - mae: 0.0185\n",
            "Epoch 23: val_mae improved from 0.03260 to 0.03254, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 4.1343e-04 - mae: 0.0185 - val_loss: 0.0011 - val_mae: 0.0325 - lr: 5.0000e-04\n",
            "Epoch 24/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 4.1007e-04 - mae: 0.0184\n",
            "Epoch 24: val_mae improved from 0.03254 to 0.03149, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 89ms/step - loss: 4.1007e-04 - mae: 0.0184 - val_loss: 0.0010 - val_mae: 0.0315 - lr: 5.0000e-04\n",
            "Epoch 25/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.9538e-04 - mae: 0.0181\n",
            "Epoch 25: val_mae did not improve from 0.03149\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 3.9538e-04 - mae: 0.0181 - val_loss: 0.0011 - val_mae: 0.0323 - lr: 5.0000e-04\n",
            "Epoch 26/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.9132e-04 - mae: 0.0179\n",
            "Epoch 26: val_mae improved from 0.03149 to 0.02900, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 88ms/step - loss: 3.9132e-04 - mae: 0.0179 - val_loss: 8.9999e-04 - val_mae: 0.0290 - lr: 5.0000e-04\n",
            "Epoch 27/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.8650e-04 - mae: 0.0179\n",
            "Epoch 27: val_mae did not improve from 0.02900\n",
            "93/93 [==============================] - 8s 82ms/step - loss: 3.8650e-04 - mae: 0.0179 - val_loss: 0.0010 - val_mae: 0.0323 - lr: 5.0000e-04\n",
            "Epoch 28/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.8021e-04 - mae: 0.0177\n",
            "Epoch 28: val_mae did not improve from 0.02900\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.8021e-04 - mae: 0.0177 - val_loss: 9.0948e-04 - val_mae: 0.0294 - lr: 5.0000e-04\n",
            "Epoch 29/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.7574e-04 - mae: 0.0175\n",
            "Epoch 29: val_mae did not improve from 0.02900\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.7574e-04 - mae: 0.0175 - val_loss: 9.8696e-04 - val_mae: 0.0313 - lr: 5.0000e-04\n",
            "Epoch 30/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.7102e-04 - mae: 0.0174\n",
            "Epoch 30: val_mae improved from 0.02900 to 0.02737, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 3.7102e-04 - mae: 0.0174 - val_loss: 7.7785e-04 - val_mae: 0.0274 - lr: 5.0000e-04\n",
            "Epoch 31/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.6802e-04 - mae: 0.0174\n",
            "Epoch 31: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.6802e-04 - mae: 0.0174 - val_loss: 9.0268e-04 - val_mae: 0.0295 - lr: 5.0000e-04\n",
            "Epoch 32/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.6541e-04 - mae: 0.0172\n",
            "Epoch 32: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 3.6541e-04 - mae: 0.0172 - val_loss: 8.6377e-04 - val_mae: 0.0296 - lr: 5.0000e-04\n",
            "Epoch 33/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.6023e-04 - mae: 0.0172\n",
            "Epoch 33: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 3.6023e-04 - mae: 0.0172 - val_loss: 0.0011 - val_mae: 0.0349 - lr: 5.0000e-04\n",
            "Epoch 34/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.5462e-04 - mae: 0.0171\n",
            "Epoch 34: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 3.5462e-04 - mae: 0.0171 - val_loss: 8.2985e-04 - val_mae: 0.0282 - lr: 5.0000e-04\n",
            "Epoch 35/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.4242e-04 - mae: 0.0167\n",
            "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 35: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 3.4242e-04 - mae: 0.0167 - val_loss: 7.9075e-04 - val_mae: 0.0279 - lr: 5.0000e-04\n",
            "Epoch 36/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.3642e-04 - mae: 0.0165\n",
            "Epoch 36: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.3642e-04 - mae: 0.0165 - val_loss: 7.8802e-04 - val_mae: 0.0277 - lr: 2.5000e-04\n",
            "Epoch 37/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.3397e-04 - mae: 0.0164\n",
            "Epoch 37: val_mae did not improve from 0.02737\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.3397e-04 - mae: 0.0164 - val_loss: 8.1970e-04 - val_mae: 0.0278 - lr: 2.5000e-04\n",
            "Epoch 38/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.2692e-04 - mae: 0.0163\n",
            "Epoch 38: val_mae improved from 0.02737 to 0.02732, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 3.2692e-04 - mae: 0.0163 - val_loss: 7.8475e-04 - val_mae: 0.0273 - lr: 2.5000e-04\n",
            "Epoch 39/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.3268e-04 - mae: 0.0164\n",
            "Epoch 39: val_mae improved from 0.02732 to 0.02701, saving model to /content/drive/MyDrive/tf_certification/model5.h5\n",
            "93/93 [==============================] - 8s 88ms/step - loss: 3.3268e-04 - mae: 0.0164 - val_loss: 8.0032e-04 - val_mae: 0.0270 - lr: 2.5000e-04\n",
            "Epoch 40/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 3.2966e-04 - mae: 0.0163\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\n",
            "Epoch 40: val_mae did not improve from 0.02701\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 3.2966e-04 - mae: 0.0163 - val_loss: 8.1400e-04 - val_mae: 0.0277 - lr: 2.5000e-04\n",
            "Epoch 40: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcfcTVAMcM_Z",
        "outputId": "e2adba43-a3c2-403d-bf3d-1a8cfbbcc471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "model = tf.keras.models.load_model(save_path)\n",
        "import math\n",
        "def model_forecast(model, series, window_size):\n",
        "   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "   ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "   ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "   ds = ds.batch(32).prefetch(1)\n",
        "   forecast = model.predict(ds)\n",
        "   return forecast\n",
        "\n",
        "split_time = 3000\n",
        "window_size = 30\n",
        "# YOUR CODE HERE\n",
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "\n",
        "# To get the maximum score, your model must have an MAE OF .12 or less.\n",
        "# When you Submit and Test your model, the grading infrastructure\n",
        "# converts the MAE of your model to a score from 0 to 5 as follows:\n",
        "\n",
        "test_val = 100 * result\n",
        "score = math.ceil(17 - test_val)\n",
        "if score > 5:\n",
        "   score = 5\n",
        "\n",
        "print(score)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient = 10\n",
        "save_path = '/content/drive/MyDrive/tf_certification/model5_1.h5'\n",
        "callbacks_list = [\n",
        "    EarlyStopping(\n",
        "        monitor='val_loss', \n",
        "        patience=patient, \n",
        "        mode='min', \n",
        "        verbose=1\n",
        "    ),\n",
        "    ReduceLROnPlateau(\n",
        "        monitor = 'val_loss', \n",
        "        factor = 0.5, \n",
        "        patience = patient / 2, \n",
        "        min_lr=1e-6,\n",
        "        verbose=1,\n",
        "        mode='min'\n",
        "    ),\n",
        "    ModelCheckpoint(save_path, \n",
        "                    monitor=\"val_mae\",\n",
        "                    mode=\"min\",\n",
        "                    verbose=1,\n",
        "                    save_best_only=True)\n",
        "    ]"
      ],
      "metadata": {
        "id": "nMoo8AiqItnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model 2 - parameter 1\n",
        "model.compile(loss=tf.keras.losses.Huber(), optimizer='adam', metrics=['mae'])\n",
        "history = model.fit(train_set, epochs=200, verbose=1,validation_data=val_set,\n",
        "                    callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_tdh2ZAKt7M",
        "outputId": "b7a593d3-2f27-40de-ef45-554b58f49c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "     93/Unknown - 16s 80ms/step - loss: 0.1879 - mae: 0.4942\n",
            "Epoch 1: val_mae improved from inf to 0.29265, saving model to /content/drive/MyDrive/tf_certification/model5_1.h5\n",
            "93/93 [==============================] - 18s 106ms/step - loss: 0.1879 - mae: 0.4942 - val_loss: 0.0775 - val_mae: 0.2927 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0753 - mae: 0.2920\n",
            "Epoch 2: val_mae improved from 0.29265 to 0.24257, saving model to /content/drive/MyDrive/tf_certification/model5_1.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 0.0753 - mae: 0.2920 - val_loss: 0.0618 - val_mae: 0.2426 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0554 - mae: 0.2461\n",
            "Epoch 3: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0554 - mae: 0.2461 - val_loss: 0.0809 - val_mae: 0.2612 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0404 - mae: 0.2065\n",
            "Epoch 4: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 0.0404 - mae: 0.2065 - val_loss: 0.0824 - val_mae: 0.2482 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0313 - mae: 0.1754\n",
            "Epoch 5: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 84ms/step - loss: 0.0313 - mae: 0.1754 - val_loss: 0.0944 - val_mae: 0.2731 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0275 - mae: 0.1621\n",
            "Epoch 6: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 9s 94ms/step - loss: 0.0275 - mae: 0.1621 - val_loss: 0.0917 - val_mae: 0.2575 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0242 - mae: 0.1482\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\n",
            "Epoch 7: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 83ms/step - loss: 0.0242 - mae: 0.1482 - val_loss: 0.0888 - val_mae: 0.2608 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0224 - mae: 0.1415\n",
            "Epoch 8: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 85ms/step - loss: 0.0224 - mae: 0.1415 - val_loss: 0.0866 - val_mae: 0.2437 - lr: 5.0000e-04\n",
            "Epoch 9/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0218 - mae: 0.1386\n",
            "Epoch 9: val_mae did not improve from 0.24257\n",
            "93/93 [==============================] - 8s 86ms/step - loss: 0.0218 - mae: 0.1386 - val_loss: 0.0905 - val_mae: 0.2480 - lr: 5.0000e-04\n",
            "Epoch 10/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0213 - mae: 0.1359\n",
            "Epoch 10: val_mae improved from 0.24257 to 0.23866, saving model to /content/drive/MyDrive/tf_certification/model5_1.h5\n",
            "93/93 [==============================] - 8s 88ms/step - loss: 0.0213 - mae: 0.1359 - val_loss: 0.0871 - val_mae: 0.2387 - lr: 5.0000e-04\n",
            "Epoch 11/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0201 - mae: 0.1323\n",
            "Epoch 11: val_mae improved from 0.23866 to 0.23303, saving model to /content/drive/MyDrive/tf_certification/model5_1.h5\n",
            "93/93 [==============================] - 8s 89ms/step - loss: 0.0201 - mae: 0.1323 - val_loss: 0.0813 - val_mae: 0.2330 - lr: 5.0000e-04\n",
            "Epoch 12/200\n",
            "93/93 [==============================] - ETA: 0s - loss: 0.0198 - mae: 0.1308\n",
            "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\n",
            "Epoch 12: val_mae improved from 0.23303 to 0.23084, saving model to /content/drive/MyDrive/tf_certification/model5_1.h5\n",
            "93/93 [==============================] - 8s 87ms/step - loss: 0.0198 - mae: 0.1308 - val_loss: 0.0799 - val_mae: 0.2308 - lr: 5.0000e-04\n",
            "Epoch 12: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "model = tf.keras.models.load_model(save_path)\n",
        "import math\n",
        "def model_forecast(model, series, window_size):\n",
        "   ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "   ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "   ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "   ds = ds.batch(32).prefetch(1)\n",
        "   forecast = model.predict(ds)\n",
        "   return forecast\n",
        "\n",
        "split_time = 3000\n",
        "window_size = 30\n",
        "# YOUR CODE HERE\n",
        "rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n",
        "rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]\n",
        "\n",
        "result = tf.keras.metrics.mean_absolute_error(x_valid, rnn_forecast).numpy()\n",
        "\n",
        "# To get the maximum score, your model must have an MAE OF .12 or less.\n",
        "# When you Submit and Test your model, the grading infrastructure\n",
        "# converts the MAE of your model to a score from 0 to 5 as follows:\n",
        "\n",
        "test_val = 100 * result\n",
        "score = math.ceil(17 - test_val)\n",
        "if score > 5:\n",
        "   score = 5\n",
        "\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxPKThX7K2Ad",
        "outputId": "a5dc42a4-8123-4bbc-e2ad-87377cc43655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EqoNmRp0L7VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## new"
      ],
      "metadata": {
        "id": "UD3mjyYQzz9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "# recurrent_dropout argument (you can alternatively set it to 0),\n",
        "# since it has not been implemented in the cuDNN kernel and may\n",
        "# result in much longer training times.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ==============================================================================\n",
        "#\n",
        "# TIME SERIES QUESTION\n",
        "#\n",
        "# Build and train a neural network to predict time indexed variables of\n",
        "# the multivariate house hold electric power consumption time series dataset.\n",
        "# Using a window of past 24 observations of the 7 variables, the model\n",
        "# should be trained to predict the next 24 observations of the 7 variables.\n",
        "#\n",
        "# ==============================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption\n",
        "#\n",
        "# The original Individual House Hold Electric Power Consumption Dataset\n",
        "# has Measurements of electric power consumption in one household with\n",
        "# a one-minute sampling rate over a period of almost 4 years.\n",
        "#\n",
        "# Different electrical quantities and some sub-metering values are available.\n",
        "#\n",
        "# For the purpose of the examination we have provided a subset containing\n",
        "# the data for the first 60 days in the dataset. We have also cleaned the\n",
        "# dataset beforehand to remove missing values. The dataset is provided as a\n",
        "# CSV file in the project.\n",
        "#\n",
        "# The dataset has a total of 7 features ordered by time.\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "#\n",
        "# 1. Model input shape must be (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7),\n",
        "#    since the testing infrastructure expects a window of past N_PAST = 24\n",
        "#    observations of the 7 features to predict the next N_FUTURE = 24\n",
        "#    observations of the same features.\n",
        "#\n",
        "# 2. Model output shape must be (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7)\n",
        "#\n",
        "# 3. The last layer of your model must be a Dense layer with 7 neurons since\n",
        "#    the model is expected to predict observations of 7 features.\n",
        "#\n",
        "# 4. Don't change the values of the following constants:\n",
        "#    SPLIT_TIME, N_FEATURES, BATCH_SIZE, N_PAST, N_FUTURE, SHIFT, in\n",
        "#    solution_model() (See code for additional note on BATCH_SIZE).\n",
        "#\n",
        "# 5. Code for normalizing the data is provided - don't change it.\n",
        "#    Changing the normalizing code will affect your score.\n",
        "#\n",
        "# 6. Code for converting the dataset into windows is provided - don't change it.\n",
        "#    Changing the windowing code will affect your score.\n",
        "#\n",
        "# 7. Code for setting the seed is provided - don't change it.\n",
        "#\n",
        "# HINT: If you follow all the rules mentioned above and throughout this\n",
        "# question while training your neural network, there is a possibility that a\n",
        "# validation MAE of approximately 0.055 or less on the normalized validation\n",
        "# dataset may fetch you top marks.\n",
        "\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "\n",
        "# This function normalizes the dataset using min max scaling.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def normalize_series(data, min, max):\n",
        "    data = data - min\n",
        "    data = data / max\n",
        "    return data\n",
        "\n",
        "\n",
        "# This function is used to map the time series dataset into windows of\n",
        "# features and respective targets, to prepare it for training and\n",
        "# validation. First element of the first window will be the first element of\n",
        "# the dataset. Consecutive windows are constructed by shifting\n",
        "# the starting position of the first window forward, one at a time (indicated\n",
        "# by shift=1). For a window of n_past number of observations of all the time\n",
        "# indexed variables in the dataset, the target for the window\n",
        "# is the next n_future number of observations of these variables, after the\n",
        "# end of the window.\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "\n",
        "# This function loads the data from CSV file, normalizes the data and\n",
        "# splits the dataset into train and validation data. It also uses\n",
        "# windowed_dataset() to split the data into windows of observations and\n",
        "# targets. Finally it defines, compiles and trains a neural network. This\n",
        "# function returns the final trained model.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "    # Reads the dataset from the CSV.\n",
        "    df = pd.read_csv('household_power_consumption.csv', sep=',',\n",
        "                     infer_datetime_format=True, index_col='datetime', header=0)\n",
        "\n",
        "    # Number of features in the dataset. We use all features as predictors to\n",
        "    # predict all features at future time steps.\n",
        "    N_FEATURES = len(df.columns) # DO NOT CHANGE THIS\n",
        "\n",
        "    # Normalizes the data\n",
        "    data = df.values\n",
        "    data = normalize_series(data, data.min(axis=0), data.max(axis=0))\n",
        "\n",
        "    # Splits the data into training and validation sets.\n",
        "    SPLIT_TIME = int(len(data) * 0.5) # DO NOT CHANGE THIS\n",
        "    x_train = data[:SPLIT_TIME]\n",
        "    x_valid = data[SPLIT_TIME:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.random.set_seed(42)\n",
        "\n",
        "    # DO NOT CHANGE BATCH_SIZE IF YOU ARE USING STATEFUL LSTM/RNN/GRU.\n",
        "    # THE TEST WILL FAIL TO GRADE YOUR SCORE IN SUCH CASES.\n",
        "    # In other cases, it is advised not to change the batch size since it\n",
        "    # might affect your final scores. While setting it to a lower size\n",
        "    # might not do any harm, higher sizes might affect your scores.\n",
        "    BATCH_SIZE = 32  # ADVISED NOT TO CHANGE THIS\n",
        "\n",
        "    # DO NOT CHANGE N_PAST, N_FUTURE, SHIFT. The tests will fail to run\n",
        "    # on the server.\n",
        "    # Number of past time steps based on which future observations should be\n",
        "    # predicted\n",
        "    N_PAST = 24  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Number of future time steps which are to be predicted.\n",
        "    N_FUTURE = 24  # DO NOT CHANGE THIS\n",
        "\n",
        "    # By how many positions the window slides to create a new window\n",
        "    # of observations.\n",
        "    SHIFT = 1  # DO NOT CHANGE THIS\n",
        "\n",
        "    # Code to create windowed train and validation datasets.\n",
        "    train_set = windowed_dataset(series=x_train, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "    valid_set = windowed_dataset(series=x_valid, batch_size=BATCH_SIZE,\n",
        "                                 n_past=N_PAST, n_future=N_FUTURE,\n",
        "                                 shift=SHIFT)\n",
        "\n",
        "    # Code to define your model.\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "        # ADD YOUR LAYERS HERE.\n",
        "\n",
        "        # If you don't follow the instructions in the following comments,\n",
        "        # tests will fail to grade your code:\n",
        "        # The input layer of your model must have an input shape of:\n",
        "        # (BATCH_SIZE, N_PAST = 24, N_FEATURES = 7)\n",
        "        # The model must have an output shape of:\n",
        "        # (BATCH_SIZE, N_FUTURE = 24, N_FEATURES = 7).\n",
        "        # Make sure that there are N_FEATURES = 7 neurons in the final dense\n",
        "        # layer since the model predicts 7 features.\n",
        "\n",
        "        # HINT: Bidirectional LSTMs may help boost your score. This is only a\n",
        "        # suggestion.\n",
        "\n",
        "        # WARNING: If you are using the GRU layer, it is advised not to use the\n",
        "        # recurrent_dropout argument (you can alternatively set it to 0),\n",
        "        # since it has not been implemented in the cuDNN kernel and may\n",
        "        # result in much longer training times.\n",
        "        # tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding='causal', activation='relu', input_shape=[None, 1]),\n",
        "        tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(N_FEATURES)\n",
        "    ])\n",
        "\n",
        "    # Code to train and compile the model\n",
        "    # optimizer =  # YOUR CODE HERE\n",
        "    model.compile(\n",
        "        # YOUR CODE HERE\n",
        "        loss=tf.keras.losses.Huber(), optimizer='adam', metrics=['mae']\n",
        "\n",
        "    )\n",
        "    patient = 10\n",
        "    save_path = '/content/drive/MyDrive/tf_certification/model5_new.h5'\n",
        "    callbacks_list = [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss', \n",
        "            patience=patient, \n",
        "            mode='min', \n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor = 'val_loss', \n",
        "            factor = 0.5, \n",
        "            patience = patient / 2, \n",
        "            min_lr=1e-6,\n",
        "            verbose=1,\n",
        "            mode='min'\n",
        "        ),\n",
        "        ModelCheckpoint(save_path, \n",
        "                        monitor=\"val_mae\",\n",
        "                        mode=\"min\",\n",
        "                        verbose=1,\n",
        "                        save_best_only=True)\n",
        "        ]\n",
        "    model.fit(train_set, epochs=200, verbose=1,validation_data=valid_set,callbacks=callbacks_list)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "\n",
        "# if _name_ == '_main_':\n",
        "model = solution_model()\n",
        "#     model.save(\"c5q4.h5\")\n",
        "\n",
        "\n",
        "# THIS CODE IS USED IN THE TESTER FOR FORECASTING. IF YOU WANT TO TEST YOUR MODEL\n",
        "# BEFORE UPLOADING YOU CAN DO IT WITH THIS\n",
        "#def mae(y_true, y_pred):\n",
        "#    return np.mean(abs(y_true.ravel() - y_pred.ravel()))\n",
        "#\n",
        "#\n",
        "#def model_forecast(model, series, window_size, batch_size):\n",
        "#    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "#    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
        "#    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
        "#    ds = ds.batch(batch_size, drop_remainder=True).prefetch(1)\n",
        "#    forecast = model.predict(ds)\n",
        "#    return forecast\n",
        "#\n",
        "\n",
        "# PASS THE NORMALIZED data IN THE FOLLOWING CODE\n",
        "\n",
        "#rnn_forecast = model_forecast(model, data, N_PAST, BATCH_SIZE)\n",
        "#rnn_forecast = rnn_forecast[SPLIT_TIME - N_PAST:-1, 0, :]\n",
        "\n",
        "#x_valid = x_valid[:rnn_forecast.shape[0]]\n",
        "#result = mae(x_valid, rnn_forecast)"
      ],
      "metadata": {
        "id": "P1jZ1-gAz0zg",
        "outputId": "a3dc82a7-504d-4b2e-8532-9b894825a21f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "   1349/Unknown - 106s 69ms/step - loss: 0.0109 - mae: 0.0850\n",
            "Epoch 00001: val_mae improved from inf to 0.06887, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 134s 90ms/step - loss: 0.0109 - mae: 0.0850 - val_loss: 0.0087 - val_mae: 0.0689 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0083 - mae: 0.0712\n",
            "Epoch 00002: val_mae improved from 0.06887 to 0.06253, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 126s 93ms/step - loss: 0.0083 - mae: 0.0712 - val_loss: 0.0074 - val_mae: 0.0625 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0084 - mae: 0.0700\n",
            "Epoch 00003: val_mae did not improve from 0.06253\n",
            "1349/1349 [==============================] - 123s 91ms/step - loss: 0.0084 - mae: 0.0700 - val_loss: 0.0077 - val_mae: 0.0637 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0076 - mae: 0.0646\n",
            "Epoch 00004: val_mae improved from 0.06253 to 0.05795, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 123s 91ms/step - loss: 0.0076 - mae: 0.0646 - val_loss: 0.0071 - val_mae: 0.0580 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0077 - mae: 0.0647\n",
            "Epoch 00005: val_mae did not improve from 0.05795\n",
            "1349/1349 [==============================] - 123s 91ms/step - loss: 0.0077 - mae: 0.0647 - val_loss: 0.0073 - val_mae: 0.0601 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0072 - mae: 0.0607\n",
            "Epoch 00006: val_mae improved from 0.05795 to 0.05743, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 126s 93ms/step - loss: 0.0072 - mae: 0.0607 - val_loss: 0.0070 - val_mae: 0.0574 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0071 - mae: 0.0594\n",
            "Epoch 00007: val_mae improved from 0.05743 to 0.05581, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 120s 89ms/step - loss: 0.0071 - mae: 0.0594 - val_loss: 0.0068 - val_mae: 0.0558 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "1349/1349 [==============================] - ETA: 0s - loss: 0.0069 - mae: 0.0583\n",
            "Epoch 00008: val_mae improved from 0.05581 to 0.05541, saving model to /content/drive/MyDrive/tf_certification/model5_new.h5\n",
            "1349/1349 [==============================] - 123s 91ms/step - loss: 0.0069 - mae: 0.0583 - val_loss: 0.0068 - val_mae: 0.0554 - lr: 0.0010\n",
            "Epoch 9/200\n",
            " 982/1349 [====================>.........] - ETA: 25s - loss: 0.0066 - mae: 0.0566"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9-C_lTNA3I4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "TF_training_Day_3.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}